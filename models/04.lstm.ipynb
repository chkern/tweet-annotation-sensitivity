{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0aaf08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec374c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"Epoch\"]\n",
    "for i in range(1,21):\n",
    "    l.append(str(i))\n",
    "train_log = pd.DataFrame(columns=l)\n",
    "\n",
    "test_csv = dict()\n",
    "test_csv_new = dict()\n",
    "for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "    # init test file\n",
    "    test_csv[test_data_version] = pd.read_csv('../data/version'+test_data_version+'_test_s.csv', header=0)\n",
    "    # create a new csv df with all the original columns\n",
    "    test_csv_new[test_data_version] = pd.DataFrame(test_csv[test_data_version], columns=[\"id\",\"version\",\"batch.tweet\",\"tweet.id\", \"tweet_hashed\", \"hate.speech\", \"offensive.language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc61e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training rate 100%, offensive.language, A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 120.53it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 407.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.6903, train acc: 0.5281\n",
      "Epoch 1: dev loss: 0.6791, dev acc: 0.6156\n",
      "*** Epoch 1: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.40it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 400.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss: 0.6754, train acc: 0.6003\n",
      "Epoch 2: dev loss: 0.6604, dev acc: 0.6356\n",
      "*** Epoch 2: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 123.71it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 407.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss: 0.6510, train acc: 0.6681\n",
      "Epoch 3: dev loss: 0.6403, dev acc: 0.6889\n",
      "*** Epoch 3: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.42it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 410.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss: 0.6149, train acc: 0.7265\n",
      "Epoch 4: dev loss: 0.5986, dev acc: 0.7111\n",
      "*** Epoch 4: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.40it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 404.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss: 0.5648, train acc: 0.7521\n",
      "Epoch 5: dev loss: 0.5536, dev acc: 0.7044\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 123.26it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 403.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss: 0.5225, train acc: 0.7743\n",
      "Epoch 6: dev loss: 0.5292, dev acc: 0.7333\n",
      "*** Epoch 6: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 123.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 407.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss: 0.4774, train acc: 0.7888\n",
      "Epoch 7: dev loss: 0.5039, dev acc: 0.7400\n",
      "*** Epoch 7: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 382.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss: 0.4403, train acc: 0.8082\n",
      "Epoch 8: dev loss: 0.4942, dev acc: 0.7667\n",
      "*** Epoch 8: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.48it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 408.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss: 0.4115, train acc: 0.8110\n",
      "Epoch 9: dev loss: 0.5100, dev acc: 0.7667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.74it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 404.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss: 0.3634, train acc: 0.8499\n",
      "Epoch 10: dev loss: 0.4927, dev acc: 0.7756\n",
      "*** Epoch 10: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 402.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss: 0.3394, train acc: 0.8555\n",
      "Epoch 11: dev loss: 0.4947, dev acc: 0.7800\n",
      "*** Epoch 11: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.23it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 407.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss: 0.3082, train acc: 0.8716\n",
      "Epoch 12: dev loss: 0.5819, dev acc: 0.7711\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.62it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 409.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss: 0.2787, train acc: 0.8927\n",
      "Epoch 13: dev loss: 0.5618, dev acc: 0.7800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.48it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 403.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss: 0.2367, train acc: 0.9088\n",
      "Epoch 14: dev loss: 0.6034, dev acc: 0.7689\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.49it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 404.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train loss: 0.2126, train acc: 0.9200\n",
      "Epoch 15: dev loss: 0.6125, dev acc: 0.7800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.98it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 406.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train loss: 0.1947, train acc: 0.9305\n",
      "Epoch 16: dev loss: 0.6348, dev acc: 0.7933\n",
      "*** Epoch 16: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.28it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 407.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train loss: 0.1702, train acc: 0.9361\n",
      "Epoch 17: dev loss: 0.7255, dev acc: 0.7844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.03it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 408.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train loss: 0.1660, train acc: 0.9339\n",
      "Epoch 18: dev loss: 0.7418, dev acc: 0.8044\n",
      "*** Epoch 18: dev acc higher than best dev acc, model saved!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 121.71it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 402.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train loss: 0.1373, train acc: 0.9477\n",
      "Epoch 19: dev loss: 0.7470, dev acc: 0.7933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 122.43it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 406.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train loss: 0.1178, train acc: 0.9594\n",
      "Epoch 20: dev loss: 0.8574, dev acc: 0.7822\n",
      "\n",
      "Training finished! Best epoch is 18, best dev acc is 0.8044, 10.926488637924194 seconds used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:00<00:00, 362.59it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 362.27it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 355.21it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 353.31it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 360.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for rate in range(100,101):\n",
    "    for label_to_class in [\"hate.speech\", \"offensive.language\"]:\n",
    "        for version in [\"A\",\"B\",\"C\", \"D\",\"E\"]:\n",
    "\n",
    "            ### import some packages\n",
    "            from collections import Counter, OrderedDict\n",
    "            from typing import Dict, Iterable, List, Optional\n",
    "            from torchtext._torchtext import Vocab as VocabPybind\n",
    "            from  torchtext.vocab import Vocab\n",
    "            def vocab(ordered_dict: Dict, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True) -> Vocab:\n",
    "                specials = specials or []\n",
    "                for token in specials:\n",
    "                    ordered_dict.pop(token, None)\n",
    "                tokens = []\n",
    "                # Save room for special tokens\n",
    "                for token, freq in ordered_dict.items():\n",
    "                    if freq >= min_freq:\n",
    "                        tokens.append(token)\n",
    "                if special_first:\n",
    "                    tokens[0:0] = specials\n",
    "                else:\n",
    "                    tokens.extend(specials)\n",
    "                return Vocab(VocabPybind(tokens, None))\n",
    "            def build_vocab_from_iterator(\n",
    "                iterator: Iterable,\n",
    "                min_freq: int = 1,\n",
    "                specials: Optional[List[str]] = None,\n",
    "                special_first: bool = True,\n",
    "                max_tokens: Optional[int] = None,) -> Vocab:\n",
    "                counter = Counter()\n",
    "                for tokens in iterator:\n",
    "                    counter.update(tokens)\n",
    "                specials = specials or []\n",
    "                # First sort by descending frequency, then lexicographically\n",
    "                sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "                if max_tokens is None:\n",
    "                    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "                else:\n",
    "                    assert len(specials) < max_tokens, \"len(specials) >= max_tokens, so the vocab will be entirely special tokens.\"\n",
    "                    ordered_dict = OrderedDict(sorted_by_freq_tuples[: max_tokens - len(specials)])\n",
    "                word_vocab = vocab(ordered_dict, min_freq=min_freq, specials=specials, special_first=special_first)\n",
    "                return word_vocab\n",
    "            \n",
    "            #### bild the model\n",
    "            class LSTM(nn.Module):\n",
    "                def __init__(self,vocab_size, emb_size, lstm_size, hidden_size, dropout):\n",
    "                    super().__init__()\n",
    "                    self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "                    self.lstm = nn.LSTM(emb_size, lstm_size, bidirectional=True, batch_first=True)\n",
    "                    self.hidden = nn.Linear(lstm_size*2, hidden_size)\n",
    "                    self.linear = nn.Linear(hidden_size, 1)\n",
    "                    self.sigmoid = nn.Sigmoid()\n",
    "                    self.dropout = nn.Dropout(dropout)\n",
    "                def forward(self, input, lengths): # input.shape: (batch_size, texts_length)\n",
    "                    emb = self.emb(input) # (batch_size, texts_length, emb_size)\n",
    "                    emb = self.dropout(emb)\n",
    "                    packed = pack_padded_sequence(emb, lengths, batch_first=True, enforce_sorted=False)  #PackedSequence: data (packed length, lstm_size*2)\n",
    "                    lstm, _ = self.lstm(packed.float())  # (batch_size, texts_length, lstm_size*2)\n",
    "                    padded, _ = pad_packed_sequence(lstm, batch_first=True)\n",
    "                    output = torch.max(padded, dim=1).values # max pooling, (batch_size, lstm_size*2)\n",
    "                    output = self.hidden(self.dropout(output)) # (batch_size, hidden_size)\n",
    "                    output = self.linear(self.dropout(output)) # (batch_size, 1)\n",
    "                    output = self.sigmoid(output)\n",
    "                    return output.squeeze() # (batch_size)\n",
    "            EPOCHS = 20\n",
    "            BATCH_SIZE = 32\n",
    "            EMB_SIZE = 512\n",
    "            LSTM_SIZE = 512\n",
    "            HIDDEN_SIZE = 256\n",
    "            DROPOUT = 0.3\n",
    "            VOCAB_SIZE=5000\n",
    "            LEARNING_RATE = 5e-05\n",
    "            DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "            \n",
    "            ### load train data, with samples\n",
    "            train_csv_full = pd.read_csv(\"../data_median/train_dev_split/version\"+version+label_to_class+'_train_median_train.csv',header=0)\n",
    "            if rate < 100:\n",
    "                split_i=round(len(train_csv_full)*rate*0.01)\n",
    "                train_csv = train_csv_full[:split_i]\n",
    "            else:\n",
    "                train_csv=train_csv_full\n",
    "\n",
    "                \n",
    "            dev_csv = pd.read_csv(\"../data_median/train_dev_split/version\"+version+label_to_class+'_train_median_dev.csv',header=0)\n",
    "            X_train = [tokenizer.tokenize(text.lower()) for text in list(train_csv[\"tweet_hashed\"])]\n",
    "            X_dev = [tokenizer.tokenize(text.lower()) for text in list(dev_csv[\"tweet_hashed\"])]\n",
    "            y_train = list(train_csv[label_to_class])\n",
    "            y_dev = list(dev_csv[label_to_class])\n",
    "            train_data = list(zip(X_train, y_train))\n",
    "            dev_data = list(zip(X_dev, y_dev))\n",
    "            vocab = build_vocab_from_iterator(X_train, max_tokens=VOCAB_SIZE, specials=[\"<unk>\", \"<pad>\"])\n",
    "            vocab.set_default_index(vocab[\"<unk>\"])  # index 0 reserved for '<unk>' as default, 1 reserved for '<pad>'\n",
    "            torch.save(vocab, \"lstm.vocab.\"+label_to_class+version)\n",
    "\n",
    "            def collate(batch, vocab, device):\n",
    "                texts, labels = zip(*batch)\n",
    "                lengths = [len(text) for text in texts]\n",
    "                word_ids = [[vocab[word] for word in text] for text in texts]\n",
    "                texts = pad_sequence([torch.LongTensor(ids) for ids in word_ids], batch_first=True, padding_value=1)\n",
    "                return texts.to(device), torch.tensor(labels).to(device), torch.LongTensor(lengths)\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True,\n",
    "                                    collate_fn=lambda batch: collate(batch, vocab, DEVICE))\n",
    "            dev_loader = DataLoader(dataset=dev_data,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    collate_fn=lambda batch: collate(batch, vocab, DEVICE))\n",
    "\n",
    "            model = LSTM(VOCAB_SIZE, EMB_SIZE, LSTM_SIZE, HIDDEN_SIZE, DROPOUT).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.BCELoss()\n",
    "\n",
    "            #best_dev_acc = 0\n",
    "            best_dev_f1 = 0\n",
    "            best_epoch = 0\n",
    "            print(f'Start training rate {rate}%, {label_to_class}, {version}...')\n",
    "            start_time = time.time()\n",
    "            #train_acc_list = [str(rate)+\"_\"+label_to_class+version+\"_train\"]\n",
    "            #dev_acc_list = [str(rate)+\"_\"+label_to_class+version+\"_dev\"]\n",
    "            train_f1_list = [str(rate)+\"_\"+label_to_class+version+\"_train\"]\n",
    "            dev_f1_list = [str(rate)+\"_\"+label_to_class+version+\"_dev\"]\n",
    "            train_loss_list = [str(rate)+\"_\"+label_to_class+version+\"_train_loss\"]\n",
    "            dev_loss_list = [str(rate)+\"_\"+label_to_class+version+\"_dev_loss\"]\n",
    "            \n",
    "            for epoch in range(EPOCHS):\n",
    "                # train\n",
    "                train_loss = 0\n",
    "                train_true_list=[]\n",
    "                train_preds_list=[]\n",
    "                train_preds_score_list=[]\n",
    "                model.train()\n",
    "                ln = 0\n",
    "                for texts, labels, lengths in tqdm(train_loader):\n",
    "                    model.zero_grad()\n",
    "                    output = model(texts, lengths)\n",
    "                    preds = torch.round(output)\n",
    "                    if labels.shape == preds.shape:\n",
    "                        loss = criterion(output, labels.float())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_loss += loss.item()\n",
    "                        train_preds_list.extend(preds.tolist())\n",
    "                        train_true_list.extend(labels.tolist())\n",
    "                        train_preds_score_list.extend(output.tolist())\n",
    "                        ln+=1\n",
    "                train_loss = train_loss / ln\n",
    "                #train_acc = accuracy_score(train_true_list,train_preds_list)\n",
    "                train_f1 = f1_score(train_true_list,train_preds_list, average=\"weighted\")\n",
    "                #train_acc_list.append(train_acc)\n",
    "                train_f1_list.append(train_f1)\n",
    "                train_loss_list.append(train_loss)\n",
    "\n",
    "                # dev\n",
    "                dev_loss = 0\n",
    "                dev_true_list=[]\n",
    "                dev_preds_list=[]\n",
    "                dev_preds_score_list=[]\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for texts, labels, lengths in tqdm(dev_loader):\n",
    "                        output = model(texts, lengths)\n",
    "                        loss = criterion(output, labels.float())\n",
    "                        preds = torch.round(output)\n",
    "                        dev_loss += loss.item()\n",
    "                        dev_preds_list.extend(preds.tolist())\n",
    "                        dev_true_list.extend(labels.tolist())\n",
    "                        dev_preds_score_list.extend(output.tolist())\n",
    "                dev_loss = dev_loss / len(dev_loader)\n",
    "                #dev_acc = accuracy_score(dev_true_list,dev_preds_list)\n",
    "                #dev_acc_list.append(dev_acc)\n",
    "                dev_f1 = f1_score(dev_true_list,dev_preds_list, average=\"weighted\")\n",
    "                dev_f1_list.append(dev_f1)\n",
    "                dev_loss_list.append(dev_loss)\n",
    "\n",
    "                #print(f'Epoch {e + 1}: train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')\n",
    "                #print(f'Epoch {e + 1}: dev loss: {dev_loss:.4f}, dev acc: {dev_acc:.4f}')\n",
    "                print(f'Epoch {e + 1}: train loss: {train_loss:.4f}, train f1: {train_f1:.4f}')\n",
    "                print(f'Epoch {e + 1}: dev loss: {dev_loss:.4f}, dev f1: {dev_f1:.4f}')\n",
    "                #if dev_acc > best_dev_acc:\n",
    "                if dev_f1 > best_dev_f1:\n",
    "                    #best_dev_acc = dev_acc\n",
    "                    best_dev_f1 = dev_f1\n",
    "                    best_epoch = epoch + 1\n",
    "                    torch.save(model, \"lstm.model.\"+label_to_class+version)\n",
    "                    print(f'*** Epoch {epoch + 1}: dev metric higher than best dev metric, model saved!')\n",
    "                print()\n",
    "            sec = time.time()-start_time\n",
    "            #print(f'Training finished! Best epoch is {best_epoch}, best dev acc is {best_dev_acc:.4f}, {sec} seconds used.')\n",
    "            print(f'Training finished! Best epoch is {best_epoch}, best dev f1 is {best_dev_f1:.4f}, {sec} seconds used.')\n",
    "\n",
    "            #train_log.loc[len(train_log)]=train_acc_list\n",
    "            #train_log.loc[len(train_log)]=dev_acc_list\n",
    "            train_log.loc[len(train_log)]=train_f1_list\n",
    "            train_log.loc[len(train_log)]=dev_f1_list\n",
    "            train_log.loc[len(train_log)]=train_loss_list\n",
    "            train_log.loc[len(train_log)]=dev_loss_list\n",
    "    \n",
    "    # test\n",
    "    for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "        BATCH_SIZE = 64\n",
    "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "        def collate_for_test(batch, vocab, device):\n",
    "            texts, labels = zip(*batch)\n",
    "            lengths = [len(text) for text in texts]\n",
    "            word_ids = [[vocab[word] for word in text] for text in texts]\n",
    "            texts = pad_sequence([torch.LongTensor(ids) for ids in word_ids], batch_first=True, padding_value=1)\n",
    "            return texts.to(device), torch.LongTensor(lengths)\n",
    "        # preprocess the test data\n",
    "        X_test = [tokenizer.tokenize(text.lower()) for text in list(test_csv_new[test_data_version][\"tweet_hashed\"])]\n",
    "        # choose trained model version to test\n",
    "        for label_to_test in [\"hate.speech\", \"offensive.language\"]:\n",
    "            for version_to_test in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "                vocab_test = torch.load(\"lstm.vocab.\"+label_to_test+version_to_test, map_location=DEVICE)\n",
    "                model_test = torch.load(\"lstm.model.\"+label_to_test+version_to_test, map_location=DEVICE)\n",
    "                y_test = list(test_csv_new[test_data_version][label_to_test])\n",
    "                test_data = list(zip(X_test, y_test))\n",
    "                test_loader = DataLoader(dataset=test_data,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=lambda batch: collate_for_test(batch, vocab_test, DEVICE))\n",
    "                model_test.eval()\n",
    "                preds_list = []\n",
    "                preds_scores = []\n",
    "                with torch.no_grad():\n",
    "                    for texts, lengths in tqdm(test_loader):\n",
    "                        output = model_test(texts, lengths)\n",
    "                        preds = torch.round(output)\n",
    "                        preds_list.extend(preds.tolist())\n",
    "                        preds_scores.extend(output.tolist())\n",
    "                preds_list = [int(i) for i in preds_list]\n",
    "                test_csv_new[test_data_version][str(rate)+\"_\"+label_to_test+\"_preds_\"+version_to_test] = preds_list\n",
    "                test_csv_new[test_data_version][str(rate)+\"_\"+label_to_test+\"_preds_\"+version_to_test+\"_scores\"]=preds_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c094cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_log.to_csv(\"lstm_train_accs_median_1to100.csv\")\n",
    "train_log.to_csv(\"lstm_train_f1_median_1to100.csv\")\n",
    "for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "    test_csv_new[test_data_version].to_csv(\"lstm_test\"+test_data_version+\"_median_1to100.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
