{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dca71ac",
   "metadata": {},
   "source": [
    "### Train and test with LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41853f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec374c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the tables for saving the results\n",
    "l = [\"Epoch\"]\n",
    "for i in range(1,21):\n",
    "    l.append(str(i))\n",
    "train_log = pd.DataFrame(columns=l)\n",
    "\n",
    "test_csv = dict()\n",
    "test_csv_new = dict()\n",
    "for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "    # init test file\n",
    "    test_csv[test_data_version] = pd.read_csv('../data/version'+test_data_version+'_test_s.csv', header=0)\n",
    "    # create a new csv df with all the original columns\n",
    "    test_csv_new[test_data_version] = pd.DataFrame(test_csv[test_data_version], columns=[\"id\",\"version\",\"batch.tweet\",\"tweet.id\", \"tweet_hashed\", \"hate.speech\", \"offensive.language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for SEED in [10,42,84,420,567,888,1100,1234,5566,7890]: ### train with seeds\n",
    "    random.seed(SEED)\n",
    "    for label_to_class in [\"hate.speech\", \"offensive.language\"]:\n",
    "        for version in [\"A\",\"B\",\"C\", \"D\",\"E\"]:\n",
    "            #### bild the model\n",
    "            class LSTM(nn.Module):\n",
    "                def __init__(self,vocab_size, emb_size, lstm_size, hidden_size, dropout):\n",
    "                    super().__init__()\n",
    "                    self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "                    self.lstm = nn.LSTM(emb_size, lstm_size, bidirectional=True, batch_first=True)\n",
    "                    self.hidden = nn.Linear(lstm_size*2, hidden_size)\n",
    "                    self.linear = nn.Linear(hidden_size, 1)\n",
    "                    self.sigmoid = nn.Sigmoid()\n",
    "                    self.dropout = nn.Dropout(dropout)\n",
    "                def forward(self, input, lengths): # input.shape: (batch_size, texts_length)\n",
    "                    emb = self.emb(input) # (batch_size, texts_length, emb_size)\n",
    "                    emb = self.dropout(emb)\n",
    "                    packed = pack_padded_sequence(emb, lengths, batch_first=True, enforce_sorted=False)  #PackedSequence: data (packed length, lstm_size*2)\n",
    "                    lstm, _ = self.lstm(packed.float())  # (batch_size, texts_length, lstm_size*2)\n",
    "                    padded, _ = pad_packed_sequence(lstm, batch_first=True)\n",
    "                    output = torch.max(padded, dim=1).values # max pooling, (batch_size, lstm_size*2)\n",
    "                    output = self.hidden(self.dropout(output)) # (batch_size, hidden_size)\n",
    "                    output = self.linear(self.dropout(output)) # (batch_size, 1)\n",
    "                    output = self.sigmoid(output)\n",
    "                    return output.squeeze() # (batch_size)\n",
    "            EPOCHS = 20\n",
    "            BATCH_SIZE = 64\n",
    "            EMB_SIZE = 512\n",
    "            LSTM_SIZE = 512\n",
    "            HIDDEN_SIZE = 256\n",
    "            DROPOUT = 0.3\n",
    "            VOCAB_SIZE=5000\n",
    "            LEARNING_RATE = 5e-05\n",
    "            DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "            train_csv= pd.read_csv(\"../data_sampled/train_dev_split/version\"+version+label_to_class+'_train_sampled_train.csv',header=0) \n",
    "            dev_csv = pd.read_csv(\"../data_sampled/train_dev_split/version\"+version+label_to_class+'_train_sampled_dev.csv',header=0)\n",
    "            X_train = [tokenizer.tokenize(text.lower()) for text in list(train_csv[\"tweet_hashed\"])]\n",
    "            X_dev = [tokenizer.tokenize(text.lower()) for text in list(dev_csv[\"tweet_hashed\"])]\n",
    "            y_train = list(train_csv[label_to_class])\n",
    "            y_dev = list(dev_csv[label_to_class])\n",
    "            train_data = list(zip(X_train, y_train))\n",
    "            dev_data = list(zip(X_dev, y_dev))\n",
    "            vocab = build_vocab_from_iterator(X_train, max_tokens=VOCAB_SIZE, specials=[\"<unk>\", \"<pad>\"])\n",
    "            vocab.set_default_index(vocab[\"<unk>\"])  # index 0 reserved for '<unk>' as default, 1 reserved for '<pad>'\n",
    "            torch.save(vocab, \"lstm.vocab.\"+label_to_class+version)\n",
    "\n",
    "            def collate(batch, vocab, device):\n",
    "                texts, labels = zip(*batch)\n",
    "                lengths = [len(text) for text in texts]\n",
    "                word_ids = [[vocab[word] for word in text] for text in texts]\n",
    "                texts = pad_sequence([torch.LongTensor(ids) for ids in word_ids], batch_first=True, padding_value=1)\n",
    "                return texts.to(device), torch.tensor(labels).to(device), torch.LongTensor(lengths)\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True,\n",
    "                                    collate_fn=lambda batch: collate(batch, vocab, DEVICE))\n",
    "            dev_loader = DataLoader(dataset=dev_data,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    collate_fn=lambda batch: collate(batch, vocab, DEVICE))\n",
    "\n",
    "            model = LSTM(VOCAB_SIZE, EMB_SIZE, LSTM_SIZE, HIDDEN_SIZE, DROPOUT).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.BCELoss()\n",
    "            best_dev_acc = 0\n",
    "            best_epoch = 0            \n",
    "            print(f'Start training seed {SEED}, {label_to_class}, {version}...')\n",
    "            train_acc_list = [\"seed\"+str(SEED)+\"_\"+label_to_class+version+\"_train\"]\n",
    "            dev_acc_list = [\"seed\"+str(SEED)+\"_\"+label_to_class+version+\"_dev\"]\n",
    "            train_loss_list = [\"seed\"+str(SEED)+\"_\"+label_to_class+version+\"_train_loss\"]\n",
    "            dev_loss_list = [\"seed\"+str(SEED)+label_to_class+version+\"_dev_loss\"]\n",
    "            \n",
    "            for epoch in range(EPOCHS):\n",
    "                ### train\n",
    "                train_loss = 0\n",
    "                train_true_list=[]\n",
    "                train_preds_list=[]\n",
    "                train_preds_score_list=[]\n",
    "                model.train()\n",
    "                ln = 0\n",
    "                for texts, labels, lengths in tqdm(train_loader):\n",
    "                    model.zero_grad()\n",
    "                    output = model(texts, lengths)\n",
    "                    preds = torch.round(output)\n",
    "                    if labels.shape == preds.shape:\n",
    "                        loss = criterion(output, labels.float())\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_loss += loss.item()\n",
    "                        train_preds_list.extend(preds.tolist())\n",
    "                        train_true_list.extend(labels.tolist())\n",
    "                        train_preds_score_list.extend(output.tolist())\n",
    "                        ln+=1\n",
    "                train_loss = train_loss / ln\n",
    "                train_loss_list.append(train_loss)\n",
    "                train_acc = accuracy_score(train_true_list,train_preds_list)\n",
    "                train_acc_list.append(train_acc)              \n",
    "                \n",
    "                ### dev\n",
    "                dev_loss = 0\n",
    "                dev_true_list=[]\n",
    "                dev_preds_list=[]\n",
    "                dev_preds_score_list=[]\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for texts, labels, lengths in tqdm(dev_loader):\n",
    "                        output = model(texts, lengths)\n",
    "                        loss = criterion(output, labels.float())\n",
    "                        preds = torch.round(output)\n",
    "                        dev_loss += loss.item()\n",
    "                        dev_preds_list.extend(preds.tolist())\n",
    "                        dev_true_list.extend(labels.tolist())\n",
    "                        dev_preds_score_list.extend(output.tolist())\n",
    "                dev_loss = dev_loss / len(dev_loader)\n",
    "                dev_loss_list.append(dev_loss)\n",
    "                dev_acc = accuracy_score(dev_true_list,dev_preds_list)\n",
    "                dev_acc_list.append(dev_acc)\n",
    "                \n",
    "\n",
    "                print(f'Epoch {epoch + 1}: train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')\n",
    "                print(f'Epoch {epoch + 1}: dev loss: {dev_loss:.4f}, dev acc: {dev_acc:.4f}')\n",
    "                if dev_acc > best_dev_acc:\n",
    "                    best_dev_acc = dev_acc\n",
    "                    best_epoch = epoch + 1\n",
    "                    torch.save(model, \"lstm.model.\"+label_to_class+version)\n",
    "                    print(f'*** Epoch {epoch + 1}: dev metric higher than best dev metric, model saved!')\n",
    "                print()\n",
    "            print(f'Training finished! Best epoch is {best_epoch}, best dev acc is {best_dev_acc:.4f}')\n",
    "\n",
    "            train_log.loc[len(train_log)]=train_acc_list\n",
    "            train_log.loc[len(train_log)]=dev_acc_list\n",
    "            train_log.loc[len(train_log)]=train_loss_list\n",
    "            train_log.loc[len(train_log)]=dev_loss_list\n",
    "    \n",
    "    ### test\n",
    "    for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "        BATCH_SIZE = 64\n",
    "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "        def collate_for_test(batch, vocab, device):\n",
    "            texts, labels = zip(*batch)\n",
    "            lengths = [len(text) for text in texts]\n",
    "            word_ids = [[vocab[word] for word in text] for text in texts]\n",
    "            texts = pad_sequence([torch.LongTensor(ids) for ids in word_ids], batch_first=True, padding_value=1)\n",
    "            return texts.to(device), torch.LongTensor(lengths)\n",
    "        # preprocess the test data\n",
    "        X_test = [tokenizer.tokenize(text.lower()) for text in list(test_csv_new[test_data_version][\"tweet_hashed\"])]\n",
    "        # choose trained model version to test\n",
    "        for label_to_test in [\"hate.speech\", \"offensive.language\"]:\n",
    "            for version_to_test in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "                vocab_test = torch.load(\"lstm.vocab.\"+label_to_test+version_to_test, map_location=DEVICE)\n",
    "                model_test = torch.load(\"lstm.model.\"+label_to_test+version_to_test, map_location=DEVICE)\n",
    "                y_test = list(test_csv_new[test_data_version][label_to_test])\n",
    "                test_data = list(zip(X_test, y_test))\n",
    "                test_loader = DataLoader(dataset=test_data,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=lambda batch: collate_for_test(batch, vocab_test, DEVICE))\n",
    "                model_test.eval()\n",
    "                preds_list = []\n",
    "                preds_scores = []\n",
    "                with torch.no_grad():\n",
    "                    for texts, lengths in tqdm(test_loader):\n",
    "                        output = model_test(texts, lengths)\n",
    "                        preds = torch.round(output)\n",
    "                        preds_list.extend(preds.tolist())\n",
    "                        preds_scores.extend(output.tolist())\n",
    "                preds_list = [int(i) for i in preds_list]\n",
    "                test_csv_new[test_data_version][\"seed\"+str(SEED)+\"_\"+label_to_test+\"_preds_\"+version_to_test] = preds_list\n",
    "                test_csv_new[test_data_version][\"seed\"+str(SEED)+\"_\"+label_to_test+\"_preds_\"+version_to_test+\"_scores\"]=preds_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32588898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train log and the test results\n",
    "train_log.to_csv(\"seed_lstm_train_accs_sampled.csv\")\n",
    "for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
    "    test_csv_new[test_data_version].to_csv(\"seed_lstm_test\"+test_data_version+\"_sampled.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
