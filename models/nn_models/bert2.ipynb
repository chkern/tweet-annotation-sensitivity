{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9BU18b13VYW",
        "outputId": "f042ad0c-ae1b-4b54-e558-f427a2197557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDzG0yQ23VYa",
        "outputId": "abd2cbdf-22db-4fe9-baec-178c739c61ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/hate_speech/models\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/hate_speech/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7fDrbNP3VYa",
        "outputId": "2c7b34e2-6d3c-48e4-ea4b-65ced1d2b315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.25.1 in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.25.1) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.25.1) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers==4.25.1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT for \"offensive.language\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nIre8WwOLla9"
      },
      "outputs": [],
      "source": [
        "label_to_class=\"offensive.language\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wiId22Up3VYb"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from transformers import DistilBertForSequenceClassification,BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBOGW4MJmfFq",
        "outputId": "d37513bd-f011-49ff-dddd-c717bec52d00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Define pretrained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',  num_labels=2)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7bx1Fzxalwlo"
      },
      "outputs": [],
      "source": [
        "csv = pd.read_csv('../data/ourdata/full_train.csv',header=0)\n",
        "# create a new csv df\n",
        "csv_new = pd.DataFrame(csv, columns=[label_to_class, \"tweet_hashed\"])\n",
        "# drop all rows that have any NaN values\n",
        "csv_new_clean = csv_new.dropna(axis=0,how=\"any\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "FFDkYUzMtxaz"
      },
      "outputs": [],
      "source": [
        "# calculate max length of the tweets\n",
        "X= list(csv_new_clean[\"tweet_hashed\"])\n",
        "max_length = 0\n",
        "for x in X:\n",
        "    ids = tokenizer.encode(x)\n",
        "    max_length = max(len(ids),max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "cqElZoZsxnjI"
      },
      "outputs": [],
      "source": [
        "y = list(csv_new_clean[label_to_class])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "2vTdQOuGtxa0"
      },
      "outputs": [],
      "source": [
        "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "iuswIaiIuieo"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=max_length)\n",
        "X_dev_tokenized = tokenizer(X_dev, padding=True, truncation=True, max_length=max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "TEwlyhtYlwlp"
      },
      "outputs": [],
      "source": [
        "# Create torch dataset\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx]).long()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "_jBnHUQzTcqm"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "dev_dataset = Dataset(X_dev_tokenized, y_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "p8nJiitXlwlq"
      },
      "outputs": [],
      "source": [
        "# Define Trainer parameters\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    #labels= np.argmax(labels, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred, average = \"weighted\")\n",
        "    precision = precision_score(y_true=labels, y_pred=pred, average = \"weighted\")\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred, average = \"weighted\")\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhAZ9Rztlwlq",
        "outputId": "8cd03f4b-7c90-4a9f-dd12-7100a6ff92eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "# Define Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    seed=0,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iP4Oy7kplwlr",
        "outputId": "18b8aa27-5e94-4ec3-f413-02cd3a94d5fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 53223\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 19959\n",
            "  Number of trainable parameters = 109483778\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5500' max='19959' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5500/19959 45:20 < 1:59:15, 2.02 it/s, Epoch 0/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.604000</td>\n",
              "      <td>0.613519</td>\n",
              "      <td>0.745378</td>\n",
              "      <td>0.759086</td>\n",
              "      <td>0.745378</td>\n",
              "      <td>0.736754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.553500</td>\n",
              "      <td>0.696246</td>\n",
              "      <td>0.573801</td>\n",
              "      <td>0.733958</td>\n",
              "      <td>0.573801</td>\n",
              "      <td>0.516235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.515600</td>\n",
              "      <td>0.467818</td>\n",
              "      <td>0.803172</td>\n",
              "      <td>0.809095</td>\n",
              "      <td>0.803172</td>\n",
              "      <td>0.800062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.504300</td>\n",
              "      <td>0.476100</td>\n",
              "      <td>0.799790</td>\n",
              "      <td>0.799488</td>\n",
              "      <td>0.799790</td>\n",
              "      <td>0.799349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.487300</td>\n",
              "      <td>0.498677</td>\n",
              "      <td>0.812040</td>\n",
              "      <td>0.812100</td>\n",
              "      <td>0.812040</td>\n",
              "      <td>0.812068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.473200</td>\n",
              "      <td>0.452309</td>\n",
              "      <td>0.809109</td>\n",
              "      <td>0.812308</td>\n",
              "      <td>0.809109</td>\n",
              "      <td>0.809530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.470700</td>\n",
              "      <td>0.465473</td>\n",
              "      <td>0.816774</td>\n",
              "      <td>0.816830</td>\n",
              "      <td>0.816774</td>\n",
              "      <td>0.816096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.476100</td>\n",
              "      <td>0.480282</td>\n",
              "      <td>0.799790</td>\n",
              "      <td>0.799834</td>\n",
              "      <td>0.799790</td>\n",
              "      <td>0.798922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.460300</td>\n",
              "      <td>0.459546</td>\n",
              "      <td>0.818954</td>\n",
              "      <td>0.818816</td>\n",
              "      <td>0.818954</td>\n",
              "      <td>0.818466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.474500</td>\n",
              "      <td>0.464104</td>\n",
              "      <td>0.822261</td>\n",
              "      <td>0.826688</td>\n",
              "      <td>0.822261</td>\n",
              "      <td>0.820069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.537300</td>\n",
              "      <td>0.497972</td>\n",
              "      <td>0.816774</td>\n",
              "      <td>0.819322</td>\n",
              "      <td>0.816774</td>\n",
              "      <td>0.817156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-500\n",
            "Configuration saved in output/checkpoint-500/config.json\n",
            "Model weights saved in output/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-1000\n",
            "Configuration saved in output/checkpoint-1000/config.json\n",
            "Model weights saved in output/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-1500\n",
            "Configuration saved in output/checkpoint-1500/config.json\n",
            "Model weights saved in output/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-2000\n",
            "Configuration saved in output/checkpoint-2000/config.json\n",
            "Model weights saved in output/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-2500\n",
            "Configuration saved in output/checkpoint-2500/config.json\n",
            "Model weights saved in output/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-3000\n",
            "Configuration saved in output/checkpoint-3000/config.json\n",
            "Model weights saved in output/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-3500\n",
            "Configuration saved in output/checkpoint-3500/config.json\n",
            "Model weights saved in output/checkpoint-3500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-4000\n",
            "Configuration saved in output/checkpoint-4000/config.json\n",
            "Model weights saved in output/checkpoint-4000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-4500\n",
            "Configuration saved in output/checkpoint-4500/config.json\n",
            "Model weights saved in output/checkpoint-4500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-5000\n",
            "Configuration saved in output/checkpoint-5000/config.json\n",
            "Model weights saved in output/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 13306\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to output/checkpoint-5500\n",
            "Configuration saved in output/checkpoint-5500/config.json\n",
            "Model weights saved in output/checkpoint-5500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from output/checkpoint-3000 (score: 0.45230913162231445).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5500, training_loss=0.5051745688698509, metrics={'train_runtime': 2721.0768, 'train_samples_per_second': 58.679, 'train_steps_per_second': 7.335, 'total_flos': 3549943692240000.0, 'train_loss': 0.5051745688698509, 'epoch': 0.83})"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train pre-trained model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "GwSC_4aulwlr"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "test_csv = pd.read_csv('../data/ourdata/full_test.csv', header=0)\n",
        "\n",
        "# create a new csv df\n",
        "test_csv_new = pd.DataFrame(test_csv, columns=[\"id\",\t\"version\",\t\"batch.tweet\", label_to_class, \"tweet.id\", \"tweet_hashed\"])\n",
        "# drop all rows that have any NaN values\n",
        "test_csv_new_clean = test_csv_new.dropna(axis=0,how=\"any\")\n",
        "\n",
        "X_test = tokenizer(list(test_csv_new_clean[\"tweet_hashed\"]), padding=True, truncation=True, max_length=max_length)\n",
        "y_test = [int(label) for label in list(test_csv_new_clean[label_to_class])]\n",
        "\n",
        "test_dataset = Dataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLKF5we8lwlr",
        "outputId": "a9831117-e70d-46f5-de5c-d9588677baa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file output/checkpoint-3000/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file output/checkpoint-3000/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at output/checkpoint-3000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Load trained model\n",
        "model_path = \"output/checkpoint-3000\"\n",
        "trained_model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALwsKLsNlwlr",
        "outputId": "f5cf5eae-9807-478b-b2b8-f4e407bad356"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "# Define tester\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"tester\",\n",
        "    dataloader_pin_memory=False\n",
        ")\n",
        "tester = Trainer(model=trained_model, args=args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "xiFTZpc9lwlr",
        "outputId": "ac87ed35-0ea8-47c8-edad-f2e0484751d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 22282\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Make prediction\n",
        "raw_pred, _, _ = tester.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ppQfVTUDlwls"
      },
      "outputs": [],
      "source": [
        "# Preprocess raw predictions\n",
        "y_pred = np.argmax(raw_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYt44NcHlwlu",
        "outputId": "5aeeba7b-0db0-4e67-bf77-c78777951a36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "non-offensive.language       0.71      0.84      0.77      9832\n",
            "    offensive.language       0.85      0.73      0.79     12450\n",
            "\n",
            "              accuracy                           0.78     22282\n",
            "             macro avg       0.78      0.78      0.78     22282\n",
            "          weighted avg       0.79      0.78      0.78     22282\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=[\"non-\"+label_to_class,label_to_class]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ALE4drYctxa6"
      },
      "outputs": [],
      "source": [
        "test_csv_new_clean.insert(6,label_to_class+\"_preds_bert_full\",y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "0hKFgC1ztxa6"
      },
      "outputs": [],
      "source": [
        "test_csv_new_clean.to_csv(\"../data/preds/\" + label_to_class+ \"_preds_bert_full.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12 (default, Oct 12 2021, 06:23:56) \n[Clang 10.0.0 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
