{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHZ2IZ_wE89h"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wiId22Up3VYb"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import nltk\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_to_class = \"hate.speech\" # enter the label to be classified\n",
        "version = \"A\" # enter the version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q_JA4Z0J3VYb"
      },
      "outputs": [],
      "source": [
        "# build the model\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self,vocab_size, emb_size, lstm_size, hidden_size, dropout):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, emb_size)\n",
        "    self.lstm = nn.LSTM(emb_size, lstm_size, bidirectional=True, batch_first=True)\n",
        "    self.hidden = nn.Linear(lstm_size*2, hidden_size)\n",
        "    self.linear = nn.Linear(hidden_size, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, lengths): # input.shape: (batch_size, texts_length)\n",
        "    emb = self.emb(input) # (batch_size, texts_length, emb_size)\n",
        "    emb = self.dropout(emb)\n",
        "    packed = pack_padded_sequence(emb, lengths, batch_first=True, enforce_sorted=False)  #PackedSequence: data (packed length, lstm_size*2)\n",
        "    lstm, _ = self.lstm(packed.float())  # (batch_size, texts_length, lstm_size*2)\n",
        "    padded, _ = pad_packed_sequence(lstm, batch_first=True)\n",
        "    output = torch.max(padded, dim=1).values # max pooling, (batch_size, lstm_size*2)\n",
        "    output = self.hidden(self.dropout(output)) # (batch_size, hidden_size)\n",
        "    output = self.linear(self.dropout(output)) # (batch_size, 1)\n",
        "    output = self.sigmoid(output)\n",
        "    return output.squeeze() # (batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T4Io0UcH3VYe"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "EMB_SIZE = 512\n",
        "LSTM_SIZE = 512\n",
        "HIDDEN_SIZE = 256\n",
        "DROPOUT = 0.3\n",
        "VOCAB_SIZE=5000\n",
        "LEARNING_RATE = 5e-05\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = nltk.tokenize.TweetTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for running the raw data\n",
        "\n",
        "\"\"\"\n",
        "# no drops\n",
        "# csv = pd.read_csv('../data/version'+version+'_train.csv',header=0)\n",
        "\n",
        "# # create a new csv df\n",
        "# csv_new = pd.DataFrame(csv, columns=[label_to_class, \"tweet_hashed\"])\n",
        "# # drop all rows that have any NaN values\n",
        "# csv_new_clean = csv_new.dropna(axis=0,how=\"any\")\n",
        "\n",
        "# drops to 3 examples per tweet\n",
        "csv = pd.read_csv('../data/version'+version+'_train_s.csv',header=0)\n",
        "# create a new csv df\n",
        "csv_new = pd.DataFrame(csv, columns=[label_to_class, \"tweet_hashed\"])\n",
        "# drop all rows that have any NaN values\n",
        "csv_new_clean = csv_new.dropna(axis=0,how=\"any\")\n",
        "# save tweets into a list\n",
        "tweet=list(csv_new_clean['tweet_hashed'])\n",
        "# count tweet freqs\n",
        "tweet_count=Counter(tweet)\n",
        "for t, c in tweet_count.items():\n",
        "    if c > 3:\n",
        "        # get the index for a specific tweet into a list\n",
        "        index = csv_new_clean[csv_new_clean.tweet_hashed == t].index.tolist()\n",
        "        # randomly choose index to drop\n",
        "        index_to_drop = random.sample(index, c-3)\n",
        "        csv_new_clean = csv_new_clean.drop(index_to_drop, axis=0)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLD_0oAU9_6B"
      },
      "outputs": [],
      "source": [
        "# Code for running the sampled data\n",
        "\n",
        "csv = pd.read_csv(\"../data_sampled/version\"+version+label_to_class+'_train_sampled.csv',header=0)\n",
        "train_csv, dev_csv = train_test_split(csv, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMb3fzud6k4G"
      },
      "outputs": [],
      "source": [
        "X_train = [tokenizer.tokenize(text.lower()) for text in list(train_csv[\"tweet_hashed\"])]\n",
        "X_dev = [tokenizer.tokenize(text.lower()) for text in list(dev_csv[\"tweet_hashed\"])]\n",
        "\n",
        "y_train = list(train_csv[label_to_class])\n",
        "y_dev = list(dev_csv[label_to_class])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQ2iOqJZ6k4H"
      },
      "outputs": [],
      "source": [
        "train_data = list(zip(X_train, y_train))\n",
        "dev_data = list(zip(X_dev, y_dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hchtnPii6k4H"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocab_from_iterator(X_train, max_tokens=VOCAB_SIZE, specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # index 0 reserved for '<unk>' as default, 1 reserved for '<pad>'\n",
        "torch.save(vocab, \"LSTM.vocab.\"+label_to_class+version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW_F8wlH9afa"
      },
      "outputs": [],
      "source": [
        "def collate(batch, vocab, device):\n",
        "    texts, labels = zip(*batch)\n",
        "    lengths = [len(text) for text in texts]\n",
        "    word_ids = [[vocab[word] for word in text] for text in texts]\n",
        "    texts = pad_sequence([torch.LongTensor(ids) for ids in word_ids], batch_first=True, padding_value=1)\n",
        "    return texts.to(device), torch.tensor(labels).to(device), torch.LongTensor(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCbeba2FV5Rh"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=lambda batch: collate(batch, vocab, DEVICE))\n",
        "dev_loader = DataLoader(dataset=dev_data,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        collate_fn=lambda batch: collate(batch, vocab, DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EKPiWpZ3VYf"
      },
      "outputs": [],
      "source": [
        "model = LSTM(VOCAB_SIZE, EMB_SIZE, LSTM_SIZE, HIDDEN_SIZE, DROPOUT).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-g2v8Gc3VYf",
        "outputId": "a70abf9a-e284-473b-bcd9-3ff1863df5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 37.81it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 138.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train loss: 0.6235, train acc: 0.6780\n",
            "Epoch 1: dev loss: 0.6184, dev acc: 0.6775\n",
            "*** Epoch 1: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 36.06it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 109.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train loss: 0.6068, train acc: 0.6859\n",
            "Epoch 2: dev loss: 0.6081, dev acc: 0.6858\n",
            "*** Epoch 2: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.57it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 145.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: train loss: 0.5888, train acc: 0.6965\n",
            "Epoch 3: dev loss: 0.5796, dev acc: 0.7004\n",
            "*** Epoch 3: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.95it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 141.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: train loss: 0.5702, train acc: 0.7142\n",
            "Epoch 4: dev loss: 0.5543, dev acc: 0.7390\n",
            "*** Epoch 4: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.81it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 131.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: train loss: 0.5405, train acc: 0.7349\n",
            "Epoch 5: dev loss: 0.5309, dev acc: 0.7437\n",
            "*** Epoch 5: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 35.93it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 109.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: train loss: 0.5154, train acc: 0.7515\n",
            "Epoch 6: dev loss: 0.5091, dev acc: 0.7656\n",
            "*** Epoch 6: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.75it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 135.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: train loss: 0.4957, train acc: 0.7695\n",
            "Epoch 7: dev loss: 0.4983, dev acc: 0.7676\n",
            "*** Epoch 7: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.52it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 134.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: train loss: 0.4689, train acc: 0.7884\n",
            "Epoch 8: dev loss: 0.5206, dev acc: 0.7576\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.59it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 135.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: train loss: 0.4556, train acc: 0.7863\n",
            "Epoch 9: dev loss: 0.5114, dev acc: 0.7587\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 35.21it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 117.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: train loss: 0.4461, train acc: 0.7925\n",
            "Epoch 10: dev loss: 0.5089, dev acc: 0.7691\n",
            "*** Epoch 10: dev acc higher than best dev acc, model saved!\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.82it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 130.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: train loss: 0.4352, train acc: 0.7999\n",
            "Epoch 11: dev loss: 0.5229, dev acc: 0.7623\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.34it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 138.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: train loss: 0.4235, train acc: 0.8053\n",
            "Epoch 12: dev loss: 0.5312, dev acc: 0.7618\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.90it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 145.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: train loss: 0.4144, train acc: 0.8104\n",
            "Epoch 13: dev loss: 0.5377, dev acc: 0.7623\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 36.12it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 115.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: train loss: 0.4098, train acc: 0.8146\n",
            "Epoch 14: dev loss: 0.5465, dev acc: 0.7587\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.65it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 141.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: train loss: 0.4007, train acc: 0.8148\n",
            "Epoch 15: dev loss: 0.5814, dev acc: 0.7514\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.85it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 142.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: train loss: 0.3979, train acc: 0.8144\n",
            "Epoch 16: dev loss: 0.5512, dev acc: 0.7623\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 39.01it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 142.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: train loss: 0.3964, train acc: 0.8128\n",
            "Epoch 17: dev loss: 0.5691, dev acc: 0.7603\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 36.61it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 117.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: train loss: 0.3943, train acc: 0.8155\n",
            "Epoch 18: dev loss: 0.5711, dev acc: 0.7624\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 38.42it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 146.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: train loss: 0.3822, train acc: 0.8179\n",
            "Epoch 19: dev loss: 0.5740, dev acc: 0.7566\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 118/118 [00:03<00:00, 39.00it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 141.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: train loss: 0.3781, train acc: 0.8186\n",
            "Epoch 20: dev loss: 0.5935, dev acc: 0.7519\n",
            "\n",
            "Training finished! Best epoch is 10, best dev acc is 0.7691, 67.89233040809631 seconds used.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "best_dev_acc = 0\n",
        "best_epoch = 0\n",
        "print(f'Start training...')\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    # train\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    model.train()\n",
        "    for texts, labels, lengths in tqdm(train_loader):\n",
        "        output = model(texts, lengths)\n",
        "        preds = torch.round(output)\n",
        "        #acc = (output.ge(0.5) == labels).sum().item() / labels.size(0)\n",
        "        acc = torch.eq(labels, preds).sum().item() / labels.size(0)\n",
        "        model.zero_grad()\n",
        "        loss = criterion(output, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc\n",
        "    train_loss, train_acc = train_loss / len(train_loader), train_acc / len(train_loader)\n",
        "    \n",
        "    # dev\n",
        "    dev_loss = 0\n",
        "    dev_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for texts, labels, lengths in tqdm(dev_loader):\n",
        "            output = model(texts, lengths)\n",
        "            loss = criterion(output, labels.float())\n",
        "            preds = torch.round(output)\n",
        "            #acc = (output.ge(0.5) == labels).sum().item() / labels.size(0)\n",
        "            acc = torch.eq(labels, preds).sum().item() / labels.size(0)\n",
        "            dev_loss += loss.item()\n",
        "            dev_acc += acc\n",
        "    dev_loss, dev_acc = dev_loss / len(dev_loader), dev_acc / len(dev_loader)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}: train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')\n",
        "    print(f'Epoch {epoch + 1}: dev loss: {dev_loss:.4f}, dev acc: {dev_acc:.4f}')\n",
        "    if dev_acc > best_dev_acc:\n",
        "        best_dev_acc = dev_acc\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save(model, \"LSTM.model.\"+label_to_class+version)\n",
        "        print(f'*** Epoch {epoch + 1}: dev acc higher than best dev acc, model saved!')\n",
        "    print()\n",
        "sec = time.time()-start_time\n",
        "print(f'Training finished! Best epoch is {best_epoch}, best dev acc is {best_dev_acc:.4f}, {sec} seconds used.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_tqNFhDZv7P"
      },
      "source": [
        "# test the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IK1fchkznxWF"
      },
      "outputs": [],
      "source": [
        "for test_data_version in [\"A\",\"B\",\"C\",\"D\",\"E\"]: \n",
        "  BATCH_SIZE = 64\n",
        "  DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  tokenizer = nltk.tokenize.TweetTokenizer()\n",
        "\n",
        "  def collate_for_test(batch, vocab, device):\n",
        "      texts, labels = zip(*batch)\n",
        "      lengths = [len(text) for text in texts]\n",
        "      word_ids = [[vocab[word] for word in text] for text in texts]\n",
        "      texts = pad_sequence([torch.LongTensor(ids) for ids in word_ids], batch_first=True, padding_value=1)\n",
        "      return texts.to(device), torch.LongTensor(lengths)\n",
        "\n",
        "  # init test file\n",
        "  test_csv = pd.read_csv('../data/version'+test_data_version+'_test_s.csv', header=0)\n",
        "\n",
        "  # create a new csv df with all the original columns\n",
        "  test_csv_new = pd.DataFrame(test_csv, columns=[\"id\",\t\"version\",\t\"batch.tweet\", \"tweet.id\", \"tweet_hashed\", \"hate.speech\", \"offensive.language\"])\n",
        "  # drop all rows that have any NaN values\n",
        "  test_csv_new = test_csv_new.dropna(axis=0,how=\"any\")\n",
        "  # preprocess the test data\n",
        "\n",
        "  X_test = [tokenizer.tokenize(text.lower()) for text in list(test_csv_new[\"tweet_hashed\"])]\n",
        "\n",
        "  # choose trained model version to test\n",
        "\n",
        "  for label_to_test in [\"hate.speech\", \"offensive.language\"]:\n",
        "    for version_to_test in [\"A\",\"B\",\"C\",\"D\",\"E\"]:\n",
        "\n",
        "      vocab_test = torch.load(\"LSTM.vocab.\"+label_to_test+version_to_test, map_location=DEVICE)\n",
        "      model_test = torch.load(\"LSTM.model.\"+label_to_test+version_to_test, map_location=DEVICE)\n",
        "      \n",
        "      y_test = list(test_csv_new[label_to_test])\n",
        "      test_data = list(zip(X_test, y_test))\n",
        "\n",
        "      test_loader = DataLoader(dataset=test_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              collate_fn=lambda batch: collate_for_test(batch, vocab_test, DEVICE))\n",
        "\n",
        "      model_test.eval()\n",
        "      preds_list = []\n",
        "      preds_scores = []\n",
        "      with torch.no_grad():\n",
        "          for texts, lengths in tqdm(test_loader):\n",
        "              output = model_test(texts, lengths)\n",
        "              preds = torch.round(output)\n",
        "              preds_list.extend(preds.tolist())\n",
        "              preds_scores.extend(output.tolist())\n",
        "\n",
        "      preds_list = [int(i) for i in preds_list]\n",
        "\n",
        "      if label_to_test == \"hate.speech\":\n",
        "        if version_to_test == \"A\":\n",
        "          column = 7\n",
        "        elif version_to_test == \"B\":\n",
        "          column = 9\n",
        "        elif version_to_test == \"C\":\n",
        "          column = 11\n",
        "        elif version_to_test == \"D\":\n",
        "          column = 13\n",
        "        elif version_to_test == \"E\":\n",
        "          column = 15\n",
        "        else:\n",
        "          raise KeyError\n",
        "      elif label_to_test == \"offensive.language\":\n",
        "        if version_to_test == \"A\":\n",
        "          column = 17\n",
        "        elif version_to_test == \"B\":\n",
        "          column = 19\n",
        "        elif version_to_test == \"C\":\n",
        "          column = 21\n",
        "        elif version_to_test == \"D\":\n",
        "          column = 23\n",
        "        elif version_to_test == \"E\":\n",
        "          column = 25\n",
        "        else:\n",
        "          raise KeyError\n",
        "      else:\n",
        "        raise KeyError\n",
        "\n",
        "      test_csv_new.insert(column,label_to_test+\"_preds_\"+version_to_test,preds_list)\n",
        "      test_csv_new.insert(column+1,label_to_test+\"_preds_\"+version_to_test+\"_scores\",preds_scores)\n",
        "\n",
        "  test_csv_new.to_csv(\"./preds_sampled/lstm_test\"+test_data_version+\"_sampled.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
